{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18fbba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs import GymEnv\n",
    "\n",
    "env = GymEnv(\"Pendulum-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb5806ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.modules import Actor, MLP, ValueOperator\n",
    "from torchrl.objectives import DDPGLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4aad8fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = env.observation_spec[\"observation\"].shape[-1]\n",
    "n_act = env.action_spec.shape[-1]\n",
    "actor = Actor(\n",
    "    MLP(in_features=n_obs, out_features=n_act, num_cells=[32,32])\n",
    ")\n",
    "value_net = ValueOperator(\n",
    "    MLP(in_features=n_obs + n_act, out_features=1, num_cells=[32, 32]),\n",
    "    in_keys=[\"observation\", \"action\"]\n",
    ")\n",
    "\n",
    "ddpg_loss = DDPGLoss(actor_network=actor, value_network=value_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01d968db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        loss_actor: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        loss_value: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        pred_value: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        pred_value_max: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        target_value: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        target_value_max: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        td_error: Tensor(shape=torch.Size([100]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manu13/anaconda3/envs/MLClass/lib/python3.9/site-packages/torchrl/objectives/common.py:40: UserWarning: No target network updater has been associated with this loss module, but target parameters have been found. While this is supported, it is expected that the target network updates will be manually performed. You can deactivate this warning by turning the RL_WARNINGS env variable to False.\n",
      "  warnings.warn(\n",
      "/Users/manu13/anaconda3/envs/MLClass/lib/python3.9/site-packages/torchrl/objectives/common.py:456: UserWarning: No target network updater has been associated with this loss module, but target parameters have been found. While this is supported, it is expected that the target network updates will be manually performed. You can deactivate this warning by turning the RL_WARNINGS env variable to False.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# run the module\n",
    "\n",
    "rollout = env.rollout(max_steps=100, policy=actor)\n",
    "loss_vals = ddpg_loss(rollout)\n",
    "print(loss_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b9de562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('loss_actor', tensor(0.0336, grad_fn=<MeanBackward0>)), ('loss_value', tensor(37.1705, grad_fn=<MeanBackward0>)), ('td_error', tensor([2.9938e-03, 6.1555e-03, 1.2844e-02, 2.7106e-02, 5.7696e-02, 1.2357e-01,\n",
       "        2.6547e-01, 5.6952e-01, 1.2126e+00, 2.5447e+00, 5.2299e+00, 1.0464e+01,\n",
       "        2.0245e+01, 3.7568e+01, 6.6202e+01, 1.0962e+02, 1.6910e+02, 2.3765e+02,\n",
       "        1.7324e+02, 1.1808e+02, 7.4906e+01, 4.4340e+01, 2.4691e+01, 1.3081e+01,\n",
       "        6.6768e+00, 3.3278e+00, 1.6468e+00, 8.3325e-01, 4.5807e-01, 3.0407e-01,\n",
       "        2.7401e-01, 3.4700e-01, 5.6797e-01, 1.0676e+00, 2.1239e+00, 4.2886e+00,\n",
       "        8.5981e+00, 1.6859e+01, 3.1895e+01, 5.7461e+01, 9.7398e+01, 1.5387e+02,\n",
       "        2.2555e+02, 1.8133e+02, 1.2609e+02, 8.1694e+01, 4.9398e+01, 2.8071e+01,\n",
       "        1.5131e+01, 7.8104e+00, 3.8968e+00, 1.8965e+00, 9.0811e-01, 4.3129e-01,\n",
       "        2.0487e-01, 9.8533e-02, 4.9079e-02, 2.6447e-02, 1.6609e-02, 1.3359e-02,\n",
       "        1.4642e-02, 2.1364e-02, 3.8040e-02, 7.5725e-02, 1.5903e-01, 3.4112e-01,\n",
       "        7.3351e-01, 1.5619e+00, 3.2644e+00, 6.6539e+00, 1.3151e+01, 2.5041e+01,\n",
       "        4.5549e+01, 7.8380e+01, 1.2635e+02, 1.8950e+02, 2.1052e+02, 1.5029e+02,\n",
       "        1.0017e+02, 6.2207e+01, 3.6160e+01, 1.9862e+01, 1.0439e+01, 5.3239e+00,\n",
       "        2.6804e+00, 1.3673e+00, 7.4341e-01, 4.7268e-01, 3.9524e-01, 4.6111e-01,\n",
       "        7.0833e-01, 1.2792e+00, 2.4857e+00, 4.9454e+00, 9.8022e+00, 1.9004e+01,\n",
       "        3.5499e+01, 6.3033e+01, 1.0515e+02, 1.6339e+02])), ('pred_value', tensor([ 0.0409,  0.0527,  0.0670,  0.0842,  0.1048,  0.1289,  0.1564,  0.1863,\n",
       "         0.2163,  0.2434,  0.2648,  0.2784,  0.2837,  0.2807,  0.2698,  0.2506,\n",
       "         0.2240,  0.1935,  0.1653,  0.1438,  0.1300,  0.1222,  0.1174,  0.1116,\n",
       "         0.1013,  0.0837,  0.0580,  0.0260, -0.0097, -0.0479, -0.0896, -0.1369,\n",
       "        -0.1910, -0.2509, -0.3121, -0.3678, -0.4122, -0.4421, -0.4579, -0.4617,\n",
       "        -0.4562, -0.4434, -0.4247, -0.4016, -0.3758, -0.3489, -0.3231, -0.3001,\n",
       "        -0.2794, -0.2579, -0.2327, -0.2032, -0.1712, -0.1392, -0.1093, -0.0822,\n",
       "        -0.0579, -0.0359, -0.0152,  0.0049,  0.0253,  0.0469,  0.0704,  0.0967,\n",
       "         0.1259,  0.1578,  0.1909,  0.2226,  0.2497,  0.2695,  0.2806,  0.2831,\n",
       "         0.2774,  0.2636,  0.2417,  0.2134,  0.1833,  0.1572,  0.1383,  0.1266,\n",
       "         0.1196,  0.1140,  0.1057,  0.0911,  0.0679,  0.0364, -0.0008, -0.0415,\n",
       "        -0.0856, -0.1353, -0.1921, -0.2545, -0.3172, -0.3729, -0.4158, -0.4437,\n",
       "        -0.4573, -0.4594, -0.4526, -0.4387], grad_fn=<SqueezeBackward1>)), ('target_value', tensor([-1.3859e-02, -2.5759e-02, -4.6328e-02, -8.0413e-02, -1.3542e-01,\n",
       "        -2.2261e-01, -3.5881e-01, -5.6839e-01, -8.8489e-01, -1.3518e+00,\n",
       "        -2.0221e+00, -2.9564e+00, -4.2158e+00, -5.8486e+00, -7.8667e+00,\n",
       "        -1.0219e+01, -1.2780e+01, -1.5222e+01, -1.2997e+01, -1.0722e+01,\n",
       "        -8.5248e+00, -6.5366e+00, -4.8516e+00, -3.5051e+00, -2.4826e+00,\n",
       "        -1.7406e+00, -1.2253e+00, -8.8678e-01, -6.8647e-01, -5.9933e-01,\n",
       "        -6.1307e-01, -7.2593e-01, -9.4467e-01, -1.2842e+00, -1.7694e+00,\n",
       "        -2.4387e+00, -3.3445e+00, -4.5481e+00, -6.1054e+00, -8.0420e+00,\n",
       "        -1.0325e+01, -1.2848e+01, -1.5443e+01, -1.3867e+01, -1.1605e+01,\n",
       "        -9.3873e+00, -7.3514e+00, -5.5983e+00, -4.1692e+00, -3.0526e+00,\n",
       "        -2.2067e+00, -1.5804e+00, -1.1241e+00, -7.9594e-01, -5.6189e-01,\n",
       "        -3.9608e-01, -2.7945e-01, -1.9849e-01, -1.4411e-01, -1.1072e-01,\n",
       "        -9.5723e-02, -9.9289e-02, -1.2459e-01, -1.7852e-01, -2.7291e-01,\n",
       "        -4.2628e-01, -6.6556e-01, -1.0272e+00, -1.5571e+00, -2.3101e+00,\n",
       "        -3.3459e+00, -4.7210e+00, -6.4716e+00, -8.5896e+00, -1.0999e+01,\n",
       "        -1.3552e+01, -1.4326e+01, -1.2102e+01, -9.8703e+00, -7.7606e+00,\n",
       "        -5.8937e+00, -4.3427e+00, -3.1252e+00, -2.2162e+00, -1.5693e+00,\n",
       "        -1.1329e+00, -8.6305e-01, -7.2900e-01, -7.1430e-01, -8.1433e-01,\n",
       "        -1.0337e+00, -1.3855e+00, -1.8938e+00, -2.5967e+00, -3.5467e+00,\n",
       "        -4.8030e+00, -6.4154e+00, -8.3987e+00, -1.0707e+01, -1.3221e+01])), ('target_value_max', tensor(-0.0139)), ('pred_value_max', tensor(0.2837))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_vals.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e44494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "for key, val in loss_vals.items():\n",
    "    if key.startswith(\"loss_\"):\n",
    "        total_loss += val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e87413c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(37.2041, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27b1c107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get trainiable parameters with parameters()\n",
    "\n",
    "from torch.optim import Adam\n",
    "optim = Adam(ddpg_loss.parameters())\n",
    "total_loss.backward()\n",
    "optim.step()\n",
    "optim.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fce30ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object LossModule.parameters at 0x168da92e0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddpg_loss.parameters()# , list(ddpg_loss.parameters()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a78bf637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizing target parameters for policy training often proves to be significantly more efficient.\n",
    "# user’s responsibility to update these values\n",
    "\n",
    "from torchrl.objectives import SoftUpdate\n",
    "\n",
    "updater = SoftUpdate(ddpg_loss, eps=0.99)\n",
    "\n",
    "# In the training loop we need to update the target parameters at each optimization step\n",
    "\n",
    "updater.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8645e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collection and Storage\n",
    "\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2f41287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader is referred to a DataCollectors\n",
    "\n",
    "# SyncDataCollector: takes size of the batch (frames_per_batch) and length of the \n",
    "# iterator (possible infinite), policy, and env\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.envs import GymEnv\n",
    "from torchrl.envs.utils import RandomPolicy\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "env = GymEnv(\"CartPole-v1\")\n",
    "env.set_seed(0)\n",
    "\n",
    "policy = RandomPolicy(env.action_spec)\n",
    "collector = SyncDataCollector(env, policy, frames_per_batch=200, total_frames=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8de05e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([200, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        collector: TensorDict(\n",
      "            fields={\n",
      "                traj_ids: Tensor(shape=torch.Size([200]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([200]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([200, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([200]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([200, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([200]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "for data in collector:\n",
    "    print(data) # May have multiple trajectories in batch since size is fixed to 200\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c862557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "# Data is augmented with collectr-specific metadata\n",
    "\n",
    "print(data[\"collector\", \"traj_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81ae8ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor data in collector:\\n    storage.store(data)\\n    for i in range(n_optim):\\n        sample = storage.sample()\\n        loss_val = loss_fn(sample)\\n        loss_val.backward()\\n        optim.step() # etc\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replay Buffers - Data is stored temporarily (ReplayBuffer)\n",
    "\n",
    "\"\"\"\n",
    "for data in collector:\n",
    "    storage.store(data)\n",
    "    for i in range(n_optim):\n",
    "        sample = storage.sample()\n",
    "        loss_val = loss_fn(sample)\n",
    "        loss_val.backward()\n",
    "        optim.step() # etc\n",
    "\"\"\"\n",
    "\n",
    "# Can edit storage type, sampling used, transforms, etc.\n",
    "# LazyMemapStorage: no neet to tell what data looks like in advance, only need to specify size of buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb2e4a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.data.replay_buffers import LazyMemmapStorage, ReplayBuffer\n",
    "\n",
    "buffer_scratch_dir = tempfile.TemporaryDirectory().name\n",
    "\n",
    "buffer = ReplayBuffer(\n",
    "    storage=LazyMemmapStorage(max_size=1000, scratch_dir=buffer_scratch_dir)\n",
    ")\n",
    "\n",
    "# Populate buffer via add() (single element) or extend() (multiple elements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "58175755",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = buffer.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "88ed8e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReplayBuffer(\n",
       "    storage=LazyMemmapStorage(\n",
       "        data=TensorDict(\n",
       "            fields={\n",
       "                action: MemoryMappedTensor(shape=torch.Size([200, 2]), device=cpu, dtype=torch.int64, is_shared=True),\n",
       "                collector: TensorDict(\n",
       "                    fields={\n",
       "                        traj_ids: MemoryMappedTensor(shape=torch.Size([200]), device=cpu, dtype=torch.int64, is_shared=True)},\n",
       "                    batch_size=torch.Size([200]),\n",
       "                    device=cpu,\n",
       "                    is_shared=False),\n",
       "                done: MemoryMappedTensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.bool, is_shared=True),\n",
       "                next: TensorDict(\n",
       "                    fields={\n",
       "                        done: MemoryMappedTensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.bool, is_shared=True),\n",
       "                        observation: MemoryMappedTensor(shape=torch.Size([200, 4]), device=cpu, dtype=torch.float32, is_shared=True),\n",
       "                        reward: MemoryMappedTensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.float32, is_shared=True),\n",
       "                        terminated: MemoryMappedTensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.bool, is_shared=True),\n",
       "                        truncated: MemoryMappedTensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.bool, is_shared=True)},\n",
       "                    batch_size=torch.Size([200]),\n",
       "                    device=cpu,\n",
       "                    is_shared=False),\n",
       "                observation: MemoryMappedTensor(shape=torch.Size([200, 4]), device=cpu, dtype=torch.float32, is_shared=True),\n",
       "                terminated: MemoryMappedTensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.bool, is_shared=True),\n",
       "                truncated: MemoryMappedTensor(shape=torch.Size([200, 1]), device=cpu, dtype=torch.bool, is_shared=True)},\n",
       "            batch_size=torch.Size([200]),\n",
       "            device=cpu,\n",
       "            is_shared=False), \n",
       "        shape=torch.Size([200]), \n",
       "        len=200, \n",
       "        max_size=1000), \n",
       "    sampler=RandomSampler(), \n",
       "    writer=RoundRobinWriter(cursor=200, full_storage=False), \n",
       "    batch_size=None, \n",
       "    collate_fn=<function _collate_id at 0x15b2bb5e0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1bf9b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(buffer) == collector.frames_per_batch # buffer has same number of elements as what we got from the collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d5e7c8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([30, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        collector: TensorDict(\n",
       "            fields={\n",
       "                traj_ids: Tensor(shape=torch.Size([30]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([30]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([30, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([30]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([30, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([30, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([30]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sampling\n",
    "\n",
    "sample = buffer.sample(batch_size=30)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9d11b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging (torchrl.record)\n",
    "\n",
    "from torchrl.record import CSVLogger\n",
    "\n",
    "logger = CSVLogger(exp_name=\"my_experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d8e39b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log values such as reward, loss value or time elapsed\n",
    "\n",
    "logger.log_scalar(\"my_scalar\", 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "61a0baf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([3, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                pixels: Tensor(shape=torch.Size([3, 400, 600, 3]), device=cpu, dtype=torch.uint8, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([3]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        pixels: Tensor(shape=torch.Size([3, 400, 600, 3]), device=cpu, dtype=torch.uint8, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([3]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "# Recording videos\n",
    "\n",
    "from torchrl.envs import GymEnv\n",
    "\n",
    "env = GymEnv(\"CartPole-v1\", from_pixels=True, pixels_only=False)\n",
    "print(env.rollout(max_steps=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a618e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs import TransformedEnv\n",
    "from torchrl.record import VideoRecorder\n",
    "\n",
    "recorder = VideoRecorder(logger, tag=\"my_video\")\n",
    "record_env = TransformedEnv(env, recorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e750200",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = record_env.rollout(max_steps=3)\n",
    "recorder.dump() # Can change video format in CSVLogger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada2e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLClass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
