{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a88ac5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs import GymEnv\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e47d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b05a3a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GymEnv(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51d3384a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Composite(\n",
       "    observation: BoundedContinuous(\n",
       "        shape=torch.Size([4]),\n",
       "        space=ContinuousBox(\n",
       "            low=Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, contiguous=True),\n",
       "            high=Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
       "        device=cpu,\n",
       "        dtype=torch.float32,\n",
       "        domain=continuous),\n",
       "    device=None,\n",
       "    shape=torch.Size([]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1d9c524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedContinuous(\n",
       "    shape=torch.Size([4]),\n",
       "    space=ContinuousBox(\n",
       "        low=Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, contiguous=True),\n",
       "        high=Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
       "    device=cpu,\n",
       "    dtype=torch.float32,\n",
       "    domain=continuous)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_spec[\"observation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a766322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4]), 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_spec[\"observation\"].shape, env.observation_spec[\"observation\"].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98b4e2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHot(\n",
       "    shape=torch.Size([2]),\n",
       "    space=CategoricalBox(n=2),\n",
       "    device=cpu,\n",
       "    dtype=torch.int64,\n",
       "    domain=discrete)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9b2240c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_spec.space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3187d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dim = env.observation_spec[\"observation\"].shape[-1]\n",
    "a_dim = env.action_spec.space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4112e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs import TransformedEnv, StepCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8805c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name=\"CartPole-v1\", device=\"cpu\", from_pixels=False):\n",
    "    env = GymEnv(env_name, device=device, from_pixels=from_pixels, pixels_only=False)\n",
    "    env = TransformedEnv(env)\n",
    "    env.append_transform(StepCounter())\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7d916a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Composite(\n",
       "    output_spec: Composite(\n",
       "        full_observation_spec: Composite(\n",
       "            observation: BoundedContinuous(\n",
       "                shape=torch.Size([4]),\n",
       "                space=ContinuousBox(\n",
       "                    low=Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, contiguous=True),\n",
       "                    high=Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
       "                device=cpu,\n",
       "                dtype=torch.float32,\n",
       "                domain=continuous),\n",
       "            device=None,\n",
       "            shape=torch.Size([])),\n",
       "        full_done_spec: Composite(\n",
       "            done: Categorical(\n",
       "                shape=torch.Size([1]),\n",
       "                space=CategoricalBox(n=2),\n",
       "                device=cpu,\n",
       "                dtype=torch.bool,\n",
       "                domain=discrete),\n",
       "            terminated: Categorical(\n",
       "                shape=torch.Size([1]),\n",
       "                space=CategoricalBox(n=2),\n",
       "                device=cpu,\n",
       "                dtype=torch.bool,\n",
       "                domain=discrete),\n",
       "            truncated: Categorical(\n",
       "                shape=torch.Size([1]),\n",
       "                space=CategoricalBox(n=2),\n",
       "                device=cpu,\n",
       "                dtype=torch.bool,\n",
       "                domain=discrete),\n",
       "            device=None,\n",
       "            shape=torch.Size([])),\n",
       "        full_reward_spec: Composite(\n",
       "            reward: UnboundedContinuous(\n",
       "                shape=torch.Size([1]),\n",
       "                space=ContinuousBox(\n",
       "                    low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
       "                    high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
       "                device=cpu,\n",
       "                dtype=torch.float32,\n",
       "                domain=continuous),\n",
       "            device=None,\n",
       "            shape=torch.Size([])),\n",
       "        device=None,\n",
       "        shape=torch.Size([])),\n",
       "    input_spec: Composite(\n",
       "        full_state_spec: Composite(\n",
       "        ,\n",
       "            device=None,\n",
       "            shape=torch.Size([])),\n",
       "        full_action_spec: Composite(\n",
       "            action: OneHot(\n",
       "                shape=torch.Size([2]),\n",
       "                space=CategoricalBox(n=2),\n",
       "                device=cpu,\n",
       "                dtype=torch.int64,\n",
       "                domain=discrete),\n",
       "            device=None,\n",
       "            shape=torch.Size([])),\n",
       "        device=None,\n",
       "        shape=torch.Size([])),\n",
       "    device=None,\n",
       "    shape=torch.Size([]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d10be6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.modules import MLP, SafeProbabilisticModule\n",
    "from torchrl.data import Composite\n",
    "\n",
    "import torch.nn as nn\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "from torch.distributions import Categorical\n",
    "from torchrl.modules.distributions import OneHotCategorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7696d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_REINFORCE_modules(p_env, device):\n",
    "\n",
    "    obs_d = p_env.observation_spec[\"observation\"].shape\n",
    "    env_specs = p_env.specs\n",
    "    act_d = env_specs[\"input_spec\", \"full_action_spec\", \"action\"].space.n\n",
    "    act_spec = env_specs[\"input_spec\", \"full_action_spec\", \"action\"]\n",
    "\n",
    "    mlp = MLP(\n",
    "        in_features=obs_d[-1],\n",
    "        activation_class=nn.ReLU,\n",
    "        out_features=act_d,\n",
    "        num_cells=[128, 128],\n",
    "        device = device\n",
    "    )\n",
    "\n",
    "    policy_net = TensorDictModule(\n",
    "        module=mlp,\n",
    "        in_keys=[\"observation\"],\n",
    "        out_keys=[\"logits\"]\n",
    "    )\n",
    "\n",
    "    actor = SafeProbabilisticModule(\n",
    "        in_keys=[\"logits\"],\n",
    "        out_keys=[\"action\"],\n",
    "        distribution_class=OneHotCategorical,\n",
    "        spec=act_spec.to(device),\n",
    "        return_log_prob=True,\n",
    "        log_prob_key=\"action_log_prob\"\n",
    "    )\n",
    "\n",
    "    actor = TensorDictSequential(policy_net, actor)\n",
    "\n",
    "    return actor\n",
    "\n",
    "def make_REINFORCE_module(env_name, device):\n",
    "    proof_environment = make_env(env_name, device=device)\n",
    "    REINFORCE_actor = make_REINFORCE_modules(proof_environment, device=device)\n",
    "    del proof_environment\n",
    "    return REINFORCE_actor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2cf1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(actor, test_env, num_episodes=3, max_steps=10_000, gamma=0.99, discounted=True): \n",
    "    test_rewards = torch.zeros(num_episodes, dtype=torch.float32) \n",
    "    actor.eval() \n",
    "    with torch.no_grad(): \n",
    "        for i in range(num_episodes): \n",
    "            td_test = test_env.rollout( policy=actor, \n",
    "                                       auto_reset=True, \n",
    "                                       auto_cast_to_device=True, \n",
    "                                       break_when_any_done=True, \n",
    "                                       max_steps=max_steps, ) \n",
    "            rewards = td_test[\"next\", \"reward\"].squeeze(-1) # [T] \n",
    "            if discounted: \n",
    "                G = 0 \n",
    "                ret = 0 \n",
    "                for r in reversed(rewards): \n",
    "                    G = r + gamma * G \n",
    "                    ret = G \n",
    "                    # final G after loop is discounted return from step 0 \n",
    "                test_rewards[i] = ret \n",
    "            else: test_rewards[i] = rewards.sum() \n",
    "    return test_rewards.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2a5caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = make_REINFORCE_module(\"CartPole-v1\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a67c8829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDictSequential(\n",
       "    module=ModuleList(\n",
       "      (0): TensorDictModule(\n",
       "          module=MLP(\n",
       "            (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=128, out_features=2, bias=True)\n",
       "          ),\n",
       "          device=cpu,\n",
       "          in_keys=['observation'],\n",
       "          out_keys=['logits'])\n",
       "      (1): SafeProbabilisticModule(\n",
       "          in_keys=['logits'],\n",
       "          out_keys=['action', 'action_log_prob'],\n",
       "          distribution_class=<class 'torchrl.modules.distributions.discrete.OneHotCategorical'>, \n",
       "          distribution_kwargs={}),\n",
       "          default_interaction_type=deterministic),\n",
       "          num_samples=None))\n",
       "    ),\n",
       "    device=cpu,\n",
       "    in_keys=['observation'],\n",
       "    out_keys=['logits', 'action', 'action_log_prob'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c526c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torchrl.envs.utils import ExplorationType, set_exploration_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "570459af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50/1000, Loss: -0.003, Avg Reward: 12.18\n",
      "Episode 100/1000, Loss: 0.011, Avg Reward: 19.59\n",
      "Episode 150/1000, Loss: 0.000, Avg Reward: 20.98\n",
      "Episode 200/1000, Loss: -0.013, Avg Reward: 45.23\n",
      "Episode 250/1000, Loss: -0.022, Avg Reward: 53.00\n",
      "Episode 300/1000, Loss: -0.046, Avg Reward: 78.34\n",
      "Episode 350/1000, Loss: 0.016, Avg Reward: 83.40\n",
      "Episode 400/1000, Loss: -0.054, Avg Reward: 76.43\n",
      "Episode 450/1000, Loss: 0.008, Avg Reward: 92.52\n",
      "Episode 500/1000, Loss: 0.009, Avg Reward: 74.32\n",
      "Episode 550/1000, Loss: 0.016, Avg Reward: 96.15\n",
      "Episode 600/1000, Loss: 0.023, Avg Reward: 97.32\n",
      "Episode 650/1000, Loss: -0.010, Avg Reward: 99.29\n",
      "Episode 700/1000, Loss: -0.002, Avg Reward: 98.02\n",
      "Episode 750/1000, Loss: -0.019, Avg Reward: 99.27\n",
      "Episode 800/1000, Loss: -0.029, Avg Reward: 94.78\n",
      "Episode 850/1000, Loss: -0.011, Avg Reward: 85.51\n",
      "Episode 900/1000, Loss: 0.021, Avg Reward: 98.97\n",
      "Episode 950/1000, Loss: 0.025, Avg Reward: 86.83\n",
      "Episode 1000/1000, Loss: -0.033, Avg Reward: 99.34\n"
     ]
    }
   ],
   "source": [
    "def train_REINFORCE(\n",
    "    env_name=\"CartPole-v1\",\n",
    "    device=\"cpu\",\n",
    "    num_episodes=1000,\n",
    "    gamma=0.99,\n",
    "    lr=5e-4,\n",
    "    log_interval=50,\n",
    "):\n",
    "\n",
    "    # Environment\n",
    "    train_env = make_env(env_name, device=device)\n",
    "    test_env = make_env(env_name, device=device)\n",
    "\n",
    "    # Actor\n",
    "    actor = make_REINFORCE_module(env_name, device).to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(actor.parameters(), lr=lr)\n",
    "    with set_exploration_type(ExplorationType.RANDOM):\n",
    "        # Training\n",
    "        for episode in range(1, num_episodes + 1):\n",
    "            actor.train()\n",
    "\n",
    "            # Rollout one episode\n",
    "            td = train_env.rollout(\n",
    "                policy=actor,\n",
    "                auto_reset=True,\n",
    "                auto_cast_to_device=True,\n",
    "                break_when_any_done=True,\n",
    "                max_steps=500\n",
    "            )\n",
    "\n",
    "            rewards = td[\"next\", \"reward\"].squeeze(-1)  # [T]\n",
    "            \n",
    "            log_probs = td.get(\"action_log_prob\")  # from SafeProbabilisticModule\n",
    "            T = rewards.shape[0]\n",
    "\n",
    "            # Compute discounted returns\n",
    "            returns = torch.zeros(T, device=device)\n",
    "            G = 0\n",
    "            for t in reversed(range(T)):\n",
    "                G = rewards[t] + gamma * G\n",
    "                returns[t] = G\n",
    "\n",
    "            # Normalize returns (optional, stabilizes training)\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "            # Compute REINFORCE loss\n",
    "            loss = -(log_probs.squeeze(-1) * returns).mean()\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Logging\n",
    "            if episode % log_interval == 0:\n",
    "            \n",
    "                avg_reward = eval_model(actor, test_env, num_episodes=5)\n",
    "                print(f\"Episode {episode}/{num_episodes}, Loss: {loss.item():.3f}, Avg Reward: {avg_reward:.2f}\")\n",
    "\n",
    "    return actor\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_actor = train_REINFORCE(env_name=\"CartPole-v1\", device=\"cpu\", num_episodes=1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e854de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-hedging3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
