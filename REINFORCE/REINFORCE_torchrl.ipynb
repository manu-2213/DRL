{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "a88ac5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs import GymEnv\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "95e47d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "b05a3a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GymEnv(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "51d3384a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Composite(\n",
       "    observation: BoundedContinuous(\n",
       "        shape=torch.Size([4]),\n",
       "        space=ContinuousBox(\n",
       "            low=Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, contiguous=True),\n",
       "            high=Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
       "        device=cpu,\n",
       "        dtype=torch.float32,\n",
       "        domain=continuous),\n",
       "    device=None,\n",
       "    shape=torch.Size([]))"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "f1d9c524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedContinuous(\n",
       "    shape=torch.Size([4]),\n",
       "    space=ContinuousBox(\n",
       "        low=Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, contiguous=True),\n",
       "        high=Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
       "    device=cpu,\n",
       "    dtype=torch.float32,\n",
       "    domain=continuous)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_spec[\"observation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "6a766322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4]), 4)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_spec[\"observation\"].shape, env.observation_spec[\"observation\"].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "98b4e2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHot(\n",
       "    shape=torch.Size([2]),\n",
       "    space=CategoricalBox(n=2),\n",
       "    device=cpu,\n",
       "    dtype=torch.int64,\n",
       "    domain=discrete)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "c9b2240c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_spec.space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "3187d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dim = env.observation_spec[\"observation\"].shape[-1]\n",
    "a_dim = env.action_spec.space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4112e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs import TransformedEnv, StepCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "8805c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name=\"CartPole-v1\", device=\"cpu\", from_pixels=False):\n",
    "    env = GymEnv(env_name, device=device, from_pixels=from_pixels, pixels_only=False)\n",
    "    env = TransformedEnv(env)\n",
    "    env.append_transform(StepCounter())\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "d7d916a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Composite(\n",
       "    output_spec: Composite(\n",
       "        full_observation_spec: Composite(\n",
       "            observation: BoundedContinuous(\n",
       "                shape=torch.Size([4]),\n",
       "                space=ContinuousBox(\n",
       "                    low=Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, contiguous=True),\n",
       "                    high=Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
       "                device=cpu,\n",
       "                dtype=torch.float32,\n",
       "                domain=continuous),\n",
       "            device=None,\n",
       "            shape=torch.Size([])),\n",
       "        full_done_spec: Composite(\n",
       "            done: Categorical(\n",
       "                shape=torch.Size([1]),\n",
       "                space=CategoricalBox(n=2),\n",
       "                device=cpu,\n",
       "                dtype=torch.bool,\n",
       "                domain=discrete),\n",
       "            terminated: Categorical(\n",
       "                shape=torch.Size([1]),\n",
       "                space=CategoricalBox(n=2),\n",
       "                device=cpu,\n",
       "                dtype=torch.bool,\n",
       "                domain=discrete),\n",
       "            truncated: Categorical(\n",
       "                shape=torch.Size([1]),\n",
       "                space=CategoricalBox(n=2),\n",
       "                device=cpu,\n",
       "                dtype=torch.bool,\n",
       "                domain=discrete),\n",
       "            device=None,\n",
       "            shape=torch.Size([])),\n",
       "        full_reward_spec: Composite(\n",
       "            reward: UnboundedContinuous(\n",
       "                shape=torch.Size([1]),\n",
       "                space=ContinuousBox(\n",
       "                    low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
       "                    high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
       "                device=cpu,\n",
       "                dtype=torch.float32,\n",
       "                domain=continuous),\n",
       "            device=None,\n",
       "            shape=torch.Size([])),\n",
       "        device=None,\n",
       "        shape=torch.Size([])),\n",
       "    input_spec: Composite(\n",
       "        full_state_spec: Composite(\n",
       "        ,\n",
       "            device=None,\n",
       "            shape=torch.Size([])),\n",
       "        full_action_spec: Composite(\n",
       "            action: OneHot(\n",
       "                shape=torch.Size([2]),\n",
       "                space=CategoricalBox(n=2),\n",
       "                device=cpu,\n",
       "                dtype=torch.int64,\n",
       "                domain=discrete),\n",
       "            device=None,\n",
       "            shape=torch.Size([])),\n",
       "        device=None,\n",
       "        shape=torch.Size([])),\n",
       "    device=None,\n",
       "    shape=torch.Size([]))"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "d10be6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.modules import MLP, SafeProbabilisticModule\n",
    "from torchrl.data import Composite\n",
    "\n",
    "import torch.nn as nn\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "7696d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_REINFORCE_modules(p_env, device):\n",
    "\n",
    "    obs_d = p_env.observation_spec[\"observation\"].shape\n",
    "    env_specs = p_env.specs\n",
    "    act_d = env_specs[\"input_spec\", \"full_action_spec\", \"action\"].space.n\n",
    "    act_spec = env_specs[\"input_spec\", \"full_action_spec\", \"action\"]\n",
    "\n",
    "    mlp = MLP(\n",
    "        in_features=obs_d[-1],\n",
    "        activation_class=nn.ReLU,\n",
    "        out_features=act_d,\n",
    "        num_cells=[128, 128],\n",
    "        device = device\n",
    "    )\n",
    "\n",
    "    policy_net = TensorDictModule(\n",
    "        module=mlp,\n",
    "        in_keys=[\"observation\"],\n",
    "        out_keys=[\"logits\"]\n",
    "    )\n",
    "\n",
    "    actor = SafeProbabilisticModule(\n",
    "        in_keys=[\"logits\"],\n",
    "        out_keys=[\"action\"],\n",
    "        distribution_class=Categorical,\n",
    "        spec=act_spec.to(device),\n",
    "        return_log_prob=True,\n",
    "        log_prob_key=\"action_log_prob\"\n",
    "    )\n",
    "\n",
    "    actor = TensorDictSequential(policy_net, actor)\n",
    "\n",
    "    return actor\n",
    "\n",
    "def make_REINFORCE_module(env_name, device):\n",
    "    proof_environment = make_env(env_name, device=device)\n",
    "    REINFORCE_actor = make_REINFORCE_modules(proof_environment, device=device)\n",
    "    del proof_environment\n",
    "    return REINFORCE_actor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "f2a5caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = make_REINFORCE_module(\"CartPole-v1\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "a67c8829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDictSequential(\n",
       "    module=ModuleList(\n",
       "      (0): TensorDictModule(\n",
       "          module=MLP(\n",
       "            (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=128, out_features=2, bias=True)\n",
       "          ),\n",
       "          device=cpu,\n",
       "          in_keys=['observation'],\n",
       "          out_keys=['logits'])\n",
       "      (1): SafeProbabilisticModule(\n",
       "          in_keys=['logits'],\n",
       "          out_keys=['action', 'action_log_prob'],\n",
       "          distribution_class=<class 'torch.distributions.categorical.Categorical'>, \n",
       "          distribution_kwargs={}),\n",
       "          default_interaction_type=deterministic),\n",
       "          num_samples=None))\n",
       "    ),\n",
       "    device=cpu,\n",
       "    in_keys=['observation'],\n",
       "    out_keys=['logits', 'action', 'action_log_prob'])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "a9f22955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(actor, test_env, num_episodes=3, max_steps=10_000, gamma=0.99, discounted=True):\n",
    "    test_rewards = torch.zeros(num_episodes, dtype=torch.float32)\n",
    "\n",
    "    actor.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_episodes):\n",
    "            td_test = test_env.rollout(\n",
    "                policy=actor,\n",
    "                auto_reset=True,\n",
    "                auto_cast_to_device=True,\n",
    "                break_when_any_done=True,\n",
    "                max_steps=max_steps,\n",
    "            )\n",
    "\n",
    "            rewards = td_test[\"next\", \"reward\"].squeeze(-1)  # [T]\n",
    "            if discounted:\n",
    "                G = 0\n",
    "                ret = 0\n",
    "                for r in reversed(rewards):\n",
    "                    G = r + gamma * G\n",
    "                    ret = G  # final G after loop is discounted return from step 0\n",
    "                test_rewards[i] = ret\n",
    "            else:\n",
    "                test_rewards[i] = rewards.sum()\n",
    "\n",
    "    return test_rewards.mean().item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "5c526c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "570459af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j0/tqzlnmks6cx221q8pl95104w0000gn/T/ipykernel_41548/4155209937.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards = torch.tensor(td[\"next\", \"reward\"].squeeze(-1))  # [T]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4664,  1.1492,  0.8289,  0.5052,  0.1784, -0.1518, -0.4853, -0.8222,\n",
      "        -1.1625, -1.5062])\n",
      "tensor([-0.1858, -0.2233, -0.2867, -0.3728, -0.4601, -0.5403, -0.6169, -0.6731,\n",
      "        -0.6752, -0.6375], grad_fn=<StackBackward0>)\n",
      "Episode 50/1000, Loss: -0.164, Avg Reward: 8.65\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4435,  1.0910,  0.7350,  0.3755,  0.0122, -0.3547, -0.7253, -1.0996,\n",
      "        -1.4777])\n",
      "tensor([-0.0357, -0.0569, -0.1002, -0.1925, -0.3444, -0.5313, -0.6466, -0.6901,\n",
      "        -0.6409], grad_fn=<StackBackward0>)\n",
      "Episode 100/1000, Loss: -0.233, Avg Reward: 9.19\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4435,  1.0910,  0.7350,  0.3755,  0.0122, -0.3547, -0.7253, -1.0996,\n",
      "        -1.4777])\n",
      "tensor([-0.0119, -0.0231, -0.0515, -0.1292, -0.3254, -0.6162, -0.6636, -0.6824,\n",
      "        -0.6875], grad_fn=<StackBackward0>)\n",
      "Episode 150/1000, Loss: -0.259, Avg Reward: 8.83\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4435,  1.0910,  0.7350,  0.3755,  0.0122, -0.3547, -0.7253, -1.0996,\n",
      "        -1.4777])\n",
      "tensor([-0.0047, -0.0106, -0.0279, -0.0857, -0.2719, -0.6515, -0.6697, -0.6681,\n",
      "        -0.6647], grad_fn=<StackBackward0>)\n",
      "Episode 200/1000, Loss: -0.262, Avg Reward: 9.20\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4435,  1.0910,  0.7350,  0.3755,  0.0122, -0.3547, -0.7253, -1.0996,\n",
      "        -1.4777])\n",
      "tensor([-0.0021, -0.0055, -0.0167, -0.0636, -0.2547, -0.6719, -0.6882, -0.6853,\n",
      "        -0.6806], grad_fn=<StackBackward0>)\n",
      "Episode 250/1000, Loss: -0.272, Avg Reward: 8.83\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4435,  1.0910,  0.7350,  0.3755,  0.0122, -0.3547, -0.7253, -1.0996,\n",
      "        -1.4777])\n",
      "tensor([-0.0011, -0.0034, -0.0117, -0.0531, -0.2524, -0.6495, -0.6534, -0.6472,\n",
      "        -0.6394], grad_fn=<StackBackward0>)\n",
      "Episode 300/1000, Loss: -0.258, Avg Reward: 9.01\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4664,  1.1492,  0.8289,  0.5052,  0.1784, -0.1518, -0.4853, -0.8222,\n",
      "        -1.1625, -1.5062])\n",
      "tensor([-0.0007, -0.0024, -0.0087, -0.0420, -0.2159, -0.6654, -0.6801, -0.6785,\n",
      "        -0.6754, -0.6708], grad_fn=<StackBackward0>)\n",
      "Episode 350/1000, Loss: -0.271, Avg Reward: 9.01\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4435,  1.0910,  0.7350,  0.3755,  0.0122, -0.3547, -0.7253, -1.0996,\n",
      "        -1.4777])\n",
      "tensor([-3.5334e-04, -1.2934e-03, -5.3482e-03, -3.1365e-02, -2.0176e-01,\n",
      "        -6.8793e-01, -6.5920e-01, -6.5509e-01, -6.5219e-01],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Episode 400/1000, Loss: -0.265, Avg Reward: 9.01\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4145,  1.0185,  0.6184,  0.2143, -0.1938, -0.6061, -1.0226, -1.4432])\n",
      "tensor([-2.2840e-04, -8.9526e-04, -4.0095e-03, -2.5675e-02, -1.9321e-01,\n",
      "        -6.8771e-01, -6.7778e-01, -6.7913e-01], grad_fn=<StackBackward0>)\n",
      "Episode 450/1000, Loss: -0.265, Avg Reward: 8.65\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4664,  1.1492,  0.8289,  0.5052,  0.1784, -0.1518, -0.4853, -0.8222,\n",
      "        -1.1625, -1.5062])\n",
      "tensor([-1.3876e-04, -5.2309e-04, -2.2185e-03, -1.2159e-02, -8.8556e-02,\n",
      "        -5.5708e-01, -6.6936e-01, -6.6631e-01, -6.6706e-01, -6.6872e-01],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Episode 500/1000, Loss: -0.272, Avg Reward: 8.46\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4664,  1.1492,  0.8289,  0.5052,  0.1784, -0.1518, -0.4853, -0.8222,\n",
      "        -1.1625, -1.5062])\n",
      "tensor([-1.0967e-04, -4.5872e-04, -2.1644e-03, -1.3662e-02, -1.1328e-01,\n",
      "        -6.4410e-01, -6.6907e-01, -6.6550e-01, -6.6109e-01, -6.5580e-01],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Episode 550/1000, Loss: -0.270, Avg Reward: 8.83\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4435,  1.0910,  0.7350,  0.3755,  0.0122, -0.3547, -0.7253, -1.0996,\n",
      "        -1.4777])\n",
      "tensor([-8.5831e-05, -4.1223e-04, -2.2788e-03, -1.8538e-02, -1.8472e-01,\n",
      "        -6.8489e-01, -6.8886e-01, -6.8974e-01, -6.9135e-01],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Episode 600/1000, Loss: -0.279, Avg Reward: 8.83\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4664,  1.1492,  0.8289,  0.5052,  0.1784, -0.1518, -0.4853, -0.8222,\n",
      "        -1.1625, -1.5062])\n",
      "tensor([-7.3910e-05, -3.5548e-04, -1.9362e-03, -1.4881e-02, -1.4854e-01,\n",
      "        -6.8439e-01, -6.9021e-01, -6.9175e-01, -6.9239e-01, -6.8947e-01],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Episode 650/1000, Loss: -0.281, Avg Reward: 8.83\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4435,  1.0910,  0.7350,  0.3755,  0.0122, -0.3547, -0.7253, -1.0996,\n",
      "        -1.4777])\n",
      "tensor([-5.6744e-05, -3.0661e-04, -1.9317e-03, -1.8499e-02, -2.1728e-01,\n",
      "        -6.9003e-01, -6.8593e-01, -6.8855e-01, -6.9181e-01],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Episode 700/1000, Loss: -0.279, Avg Reward: 8.46\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4435,  1.0910,  0.7350,  0.3755,  0.0122, -0.3547, -0.7253, -1.0996,\n",
      "        -1.4777])\n",
      "tensor([-4.9591e-05, -2.8586e-04, -1.9281e-03, -1.9656e-02, -2.3595e-01,\n",
      "        -6.6898e-01, -6.6776e-01, -6.6530e-01, -6.6166e-01],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Episode 750/1000, Loss: -0.269, Avg Reward: 9.19\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4664,  1.1492,  0.8289,  0.5052,  0.1784, -0.1518, -0.4853, -0.8222,\n",
      "        -1.1625, -1.5062])\n",
      "tensor([-2.2888e-05, -1.2589e-04, -7.9966e-04, -6.7377e-03, -8.2962e-02,\n",
      "        -6.5584e-01, -6.8520e-01, -6.8480e-01, -6.8381e-01, -6.8201e-01],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Episode 800/1000, Loss: -0.280, Avg Reward: 8.83\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4664,  1.1492,  0.8289,  0.5052,  0.1784, -0.1518, -0.4853, -0.8222,\n",
      "        -1.1625, -1.5062])\n",
      "tensor([-2.0981e-05, -1.2112e-04, -8.0371e-04, -7.1778e-03, -9.3656e-02,\n",
      "        -6.5493e-01, -6.6702e-01, -6.6266e-01, -6.5760e-01, -6.5161e-01],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Episode 850/1000, Loss: -0.269, Avg Reward: 8.64\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4145,  1.0185,  0.6184,  0.2143, -0.1938, -0.6061, -1.0226, -1.4432])\n",
      "tensor([-1.7643e-05, -1.2016e-04, -9.7895e-04, -1.1903e-02, -2.0153e-01,\n",
      "        -6.9273e-01, -6.9103e-01, -6.9177e-01], grad_fn=<StackBackward0>)\n",
      "Episode 900/1000, Loss: -0.270, Avg Reward: 8.65\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4664,  1.1492,  0.8289,  0.5052,  0.1784, -0.1518, -0.4853, -0.8222,\n",
      "        -1.1625, -1.5062])\n",
      "tensor([-1.0967e-05, -6.7711e-05, -4.8208e-04, -4.4339e-03, -6.2165e-02,\n",
      "        -6.0118e-01, -6.9161e-01, -6.9107e-01, -6.8909e-01, -6.8627e-01],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Episode 950/1000, Loss: -0.282, Avg Reward: 9.20\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4435,  1.0910,  0.7350,  0.3755,  0.0122, -0.3547, -0.7253, -1.0996,\n",
      "        -1.4777])\n",
      "tensor([-1.0967e-05, -8.1062e-05, -7.1502e-04, -9.6395e-03, -1.8014e-01,\n",
      "        -6.9243e-01, -6.8695e-01, -6.8693e-01, -6.8725e-01],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Episode 1000/1000, Loss: -0.279, Avg Reward: 9.38\n"
     ]
    }
   ],
   "source": [
    "def train_REINFORCE(\n",
    "    env_name=\"CartPole-v1\",\n",
    "    device=\"cpu\",\n",
    "    num_episodes=1000,\n",
    "    gamma=0.99,\n",
    "    lr=5e-4,\n",
    "    log_interval=50,\n",
    "):\n",
    "\n",
    "    # Environment\n",
    "    train_env = make_env(env_name, device=device)\n",
    "    test_env = make_env(env_name, device=device)\n",
    "\n",
    "    # Actor\n",
    "    actor = make_REINFORCE_module(env_name, device).to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(actor.parameters(), lr=lr)\n",
    "\n",
    "    # Training\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        actor.train()\n",
    "\n",
    "        # Rollout one episode\n",
    "        td = train_env.rollout(\n",
    "            policy=actor,\n",
    "            auto_reset=True,\n",
    "            auto_cast_to_device=True,\n",
    "            break_when_any_done=True,\n",
    "            max_steps=500\n",
    "        )\n",
    "\n",
    "        rewards = torch.tensor(td[\"next\", \"reward\"].squeeze(-1))  # [T]\n",
    "        \n",
    "        log_probs = td.get(\"action_log_prob\")  # from SafeProbabilisticModule\n",
    "        T = rewards.shape[0]\n",
    "\n",
    "        # Compute discounted returns\n",
    "        returns = torch.zeros(T, device=device)\n",
    "        G = 0\n",
    "        for t in reversed(range(T)):\n",
    "            G = rewards[t] + gamma * G\n",
    "            returns[t] = G\n",
    "\n",
    "        # Normalize returns (optional, stabilizes training)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        # Compute REINFORCE loss\n",
    "        loss = -(log_probs.squeeze(-1) * returns).mean()\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging\n",
    "        if episode % log_interval == 0:\n",
    "            print(rewards)\n",
    "            print(returns)\n",
    "            print(log_probs)\n",
    "            avg_reward = eval_model(actor, test_env, num_episodes=5)\n",
    "            print(f\"Episode {episode}/{num_episodes}, Loss: {loss.item():.3f}, Avg Reward: {avg_reward:.2f}\")\n",
    "\n",
    "    return actor\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_actor = train_REINFORCE(env_name=\"CartPole-v1\", device=\"cpu\", num_episodes=1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "0de94097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of TensorDictSequential(\n",
       "    module=ModuleList(\n",
       "      (0): TensorDictModule(\n",
       "          module=MLP(\n",
       "            (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=128, out_features=2, bias=True)\n",
       "          ),\n",
       "          device=cpu,\n",
       "          in_keys=['observation'],\n",
       "          out_keys=['logits'])\n",
       "      (1): SafeProbabilisticModule(\n",
       "          in_keys=['logits'],\n",
       "          out_keys=['action', 'action_log_prob'],\n",
       "          distribution_class=<class 'torch.distributions.categorical.Categorical'>, \n",
       "          distribution_kwargs={}),\n",
       "          default_interaction_type=deterministic),\n",
       "          num_samples=None))\n",
       "    ),\n",
       "    device=cpu,\n",
       "    in_keys=['observation'],\n",
       "    out_keys=['logits', 'action', 'action_log_prob'])>"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e854de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-hedging3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
