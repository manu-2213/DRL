{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "a88ac5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs import GymEnv\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "95e47d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "b05a3a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GymEnv(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "51d3384a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Composite(\n",
       "    observation: BoundedContinuous(\n",
       "        shape=torch.Size([4]),\n",
       "        space=ContinuousBox(\n",
       "            low=Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, contiguous=True),\n",
       "            high=Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
       "        device=cpu,\n",
       "        dtype=torch.float32,\n",
       "        domain=continuous),\n",
       "    device=None,\n",
       "    shape=torch.Size([]))"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "f1d9c524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedContinuous(\n",
       "    shape=torch.Size([4]),\n",
       "    space=ContinuousBox(\n",
       "        low=Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, contiguous=True),\n",
       "        high=Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
       "    device=cpu,\n",
       "    dtype=torch.float32,\n",
       "    domain=continuous)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_spec[\"observation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "6a766322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4]), 4)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_spec[\"observation\"].shape, env.observation_spec[\"observation\"].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "98b4e2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHot(\n",
       "    shape=torch.Size([2]),\n",
       "    space=CategoricalBox(n=2),\n",
       "    device=cpu,\n",
       "    dtype=torch.int64,\n",
       "    domain=discrete)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "c9b2240c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_spec.space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "3187d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dim = env.observation_spec[\"observation\"].shape[-1]\n",
    "a_dim = env.action_spec.space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "4112e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs import TransformedEnv, StepCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "8805c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name=\"CartPole-v1\", device=\"cpu\", from_pixels=False):\n",
    "    env = GymEnv(env_name, device=device, from_pixels=from_pixels, pixels_only=False)\n",
    "    env = TransformedEnv(env)\n",
    "    env.append_transform(StepCounter())\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "d7d916a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Composite(\n",
       "    output_spec: Composite(\n",
       "        full_observation_spec: Composite(\n",
       "            observation: BoundedContinuous(\n",
       "                shape=torch.Size([4]),\n",
       "                space=ContinuousBox(\n",
       "                    low=Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, contiguous=True),\n",
       "                    high=Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
       "                device=cpu,\n",
       "                dtype=torch.float32,\n",
       "                domain=continuous),\n",
       "            device=None,\n",
       "            shape=torch.Size([])),\n",
       "        full_done_spec: Composite(\n",
       "            done: Categorical(\n",
       "                shape=torch.Size([1]),\n",
       "                space=CategoricalBox(n=2),\n",
       "                device=cpu,\n",
       "                dtype=torch.bool,\n",
       "                domain=discrete),\n",
       "            terminated: Categorical(\n",
       "                shape=torch.Size([1]),\n",
       "                space=CategoricalBox(n=2),\n",
       "                device=cpu,\n",
       "                dtype=torch.bool,\n",
       "                domain=discrete),\n",
       "            truncated: Categorical(\n",
       "                shape=torch.Size([1]),\n",
       "                space=CategoricalBox(n=2),\n",
       "                device=cpu,\n",
       "                dtype=torch.bool,\n",
       "                domain=discrete),\n",
       "            device=None,\n",
       "            shape=torch.Size([])),\n",
       "        full_reward_spec: Composite(\n",
       "            reward: UnboundedContinuous(\n",
       "                shape=torch.Size([1]),\n",
       "                space=ContinuousBox(\n",
       "                    low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
       "                    high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
       "                device=cpu,\n",
       "                dtype=torch.float32,\n",
       "                domain=continuous),\n",
       "            device=None,\n",
       "            shape=torch.Size([])),\n",
       "        device=None,\n",
       "        shape=torch.Size([])),\n",
       "    input_spec: Composite(\n",
       "        full_state_spec: Composite(\n",
       "        ,\n",
       "            device=None,\n",
       "            shape=torch.Size([])),\n",
       "        full_action_spec: Composite(\n",
       "            action: OneHot(\n",
       "                shape=torch.Size([2]),\n",
       "                space=CategoricalBox(n=2),\n",
       "                device=cpu,\n",
       "                dtype=torch.int64,\n",
       "                domain=discrete),\n",
       "            device=None,\n",
       "            shape=torch.Size([])),\n",
       "        device=None,\n",
       "        shape=torch.Size([])),\n",
       "    device=None,\n",
       "    shape=torch.Size([]))"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "d10be6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.modules import MLP, SafeProbabilisticModule\n",
    "from torchrl.data import Composite\n",
    "\n",
    "import torch.nn as nn\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "7696d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_REINFORCE_modules(p_env, device):\n",
    "\n",
    "    obs_d = p_env.observation_spec[\"observation\"].shape\n",
    "    env_specs = p_env.specs\n",
    "    act_d = env_specs[\"input_spec\", \"full_action_spec\", \"action\"].space.n\n",
    "    act_spec = env_specs[\"input_spec\", \"full_action_spec\", \"action\"]\n",
    "\n",
    "    mlp = MLP(\n",
    "        in_features=obs_d[-1],\n",
    "        activation_class=nn.ReLU,\n",
    "        out_features=act_d,\n",
    "        num_cells=[128, 128],\n",
    "        device = device\n",
    "    )\n",
    "\n",
    "    policy_net = TensorDictModule(\n",
    "        module=mlp,\n",
    "        in_keys=[\"observation\"],\n",
    "        out_keys=[\"logits\"]\n",
    "    )\n",
    "\n",
    "    actor = SafeProbabilisticModule(\n",
    "        in_keys=[\"logits\"],\n",
    "        out_keys=[\"action\"],\n",
    "        distribution_class=Categorical,\n",
    "        spec=act_spec.to(device),\n",
    "        return_log_prob=True,\n",
    "        log_prob_key=\"action_log_prob\"\n",
    "    )\n",
    "\n",
    "    actor = TensorDictSequential(policy_net, actor)\n",
    "\n",
    "    return actor\n",
    "\n",
    "def make_REINFORCE_module(env_name, device):\n",
    "    proof_environment = make_env(env_name, device=device)\n",
    "    REINFORCE_actor = make_REINFORCE_modules(proof_environment, device=device)\n",
    "    del proof_environment\n",
    "    return REINFORCE_actor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "f2a5caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = make_REINFORCE_module(\"CartPole-v1\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "a67c8829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDictSequential(\n",
       "    module=ModuleList(\n",
       "      (0): TensorDictModule(\n",
       "          module=MLP(\n",
       "            (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=128, out_features=2, bias=True)\n",
       "          ),\n",
       "          device=cpu,\n",
       "          in_keys=['observation'],\n",
       "          out_keys=['logits'])\n",
       "      (1): SafeProbabilisticModule(\n",
       "          in_keys=['logits'],\n",
       "          out_keys=['action', 'action_log_prob'],\n",
       "          distribution_class=<class 'torch.distributions.categorical.Categorical'>, \n",
       "          distribution_kwargs={}),\n",
       "          default_interaction_type=deterministic),\n",
       "          num_samples=None))\n",
       "    ),\n",
       "    device=cpu,\n",
       "    in_keys=['observation'],\n",
       "    out_keys=['logits', 'action', 'action_log_prob'])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "a9f22955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(actor, test_env, num_episodes=3, max_steps=10_000, gamma=0.99, discounted=True):\n",
    "    test_rewards = torch.zeros(num_episodes, dtype=torch.float32)\n",
    "\n",
    "    actor.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_episodes):\n",
    "            td_test = test_env.rollout(\n",
    "                policy=actor,\n",
    "                auto_reset=True,\n",
    "                auto_cast_to_device=True,\n",
    "                break_when_any_done=True,\n",
    "                max_steps=max_steps,\n",
    "            )\n",
    "\n",
    "            rewards = td_test[\"next\", \"reward\"].squeeze(-1)  # [T]\n",
    "            if discounted:\n",
    "                G = 0\n",
    "                ret = 0\n",
    "                for r in reversed(rewards):\n",
    "                    G = r + gamma * G\n",
    "                    ret = G  # final G after loop is discounted return from step 0\n",
    "                test_rewards[i] = ret\n",
    "            else:\n",
    "                test_rewards[i] = rewards.sum()\n",
    "\n",
    "    return test_rewards.mean().item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "5c526c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "570459af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4849,  1.1968,  0.9058,  0.6119,  0.3150,  0.0151, -0.2878, -0.5937,\n",
      "        -0.9028, -1.2150, -1.5303])\n",
      "tensor([-0.2314, -0.2688, -0.3136, -0.3754, -0.4426, -0.5221, -0.6046, -0.6773,\n",
      "        -0.6651, -0.6325, -0.6021], grad_fn=<StackBackward0>)\n",
      "Episode 50/1000, Loss: -0.140, Avg Reward: 8.83\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4145,  1.0185,  0.6184,  0.2143, -0.1938, -0.6061, -1.0226, -1.4432])\n",
      "tensor([-0.0472, -0.0700, -0.1071, -0.1827, -0.2951, -0.4601, -0.6088, -0.6463],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Episode 100/1000, Loss: -0.206, Avg Reward: 8.65\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4664,  1.1492,  0.8289,  0.5052,  0.1784, -0.1518, -0.4853, -0.8222,\n",
      "        -1.1625, -1.5062])\n",
      "tensor([-0.0141, -0.0255, -0.0474, -0.1052, -0.2284, -0.4472, -0.6347, -0.6638,\n",
      "        -0.6802, -0.6896], grad_fn=<StackBackward0>)\n",
      "Episode 150/1000, Loss: -0.257, Avg Reward: 8.46\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4435,  1.0910,  0.7350,  0.3755,  0.0122, -0.3547, -0.7253, -1.0996,\n",
      "        -1.4777])\n",
      "tensor([-0.0061, -0.0136, -0.0325, -0.0947, -0.2821, -0.6327, -0.6893, -0.6853,\n",
      "        -0.6858], grad_fn=<StackBackward0>)\n",
      "Episode 200/1000, Loss: -0.267, Avg Reward: 8.83\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4145,  1.0185,  0.6184,  0.2143, -0.1938, -0.6061, -1.0226, -1.4432])\n",
      "tensor([-0.0029, -0.0072, -0.0197, -0.0741, -0.2823, -0.6725, -0.6923, -0.6920],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Episode 250/1000, Loss: -0.266, Avg Reward: 9.01\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4849,  1.1968,  0.9058,  0.6119,  0.3150,  0.0151, -0.2878, -0.5937,\n",
      "        -0.9028, -1.2150, -1.5303])\n",
      "tensor([-0.0012, -0.0032, -0.0086, -0.0308, -0.1317, -0.4747, -0.6791, -0.6892,\n",
      "        -0.6873, -0.6833, -0.6736], grad_fn=<StackBackward0>)\n",
      "Episode 300/1000, Loss: -0.273, Avg Reward: 9.01\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4145,  1.0185,  0.6184,  0.2143, -0.1938, -0.6061, -1.0226, -1.4432])\n",
      "tensor([-0.0007, -0.0023, -0.0074, -0.0346, -0.1884, -0.6537, -0.6873, -0.6866],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Episode 350/1000, Loss: -0.264, Avg Reward: 8.83\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4145,  1.0185,  0.6184,  0.2143, -0.1938, -0.6061, -1.0226, -1.4432])\n",
      "tensor([-6.5041e-04, -2.0783e-03, -7.6482e-03, -4.4228e-02, -2.8291e-01,\n",
      "        -6.7238e-01, -6.7997e-01, -6.7705e-01], grad_fn=<StackBackward0>)\n",
      "Episode 400/1000, Loss: -0.265, Avg Reward: 8.64\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4435,  1.0910,  0.7350,  0.3755,  0.0122, -0.3547, -0.7253, -1.0996,\n",
      "        -1.4777])\n",
      "tensor([-3.5882e-04, -1.2181e-03, -4.4971e-03, -2.4722e-02, -1.7841e-01,\n",
      "        -6.7576e-01, -6.8642e-01, -6.8827e-01, -6.9073e-01],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Episode 450/1000, Loss: -0.278, Avg Reward: 9.56\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4849,  1.1968,  0.9058,  0.6119,  0.3150,  0.0151, -0.2878, -0.5937,\n",
      "        -0.9028, -1.2150, -1.5303])\n",
      "tensor([-2.3222e-04, -7.8917e-04, -2.8770e-03, -1.5195e-02, -1.1014e-01,\n",
      "        -6.1983e-01, -6.6732e-01, -6.6587e-01, -6.6568e-01, -6.6620e-01,\n",
      "        -6.7207e-01], grad_fn=<StackBackward0>)\n",
      "Episode 500/1000, Loss: -0.270, Avg Reward: 9.01\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4664,  1.1492,  0.8289,  0.5052,  0.1784, -0.1518, -0.4853, -0.8222,\n",
      "        -1.1625, -1.5062])\n",
      "tensor([-1.4544e-04, -5.4121e-04, -2.1656e-03, -1.2752e-02, -1.0944e-01,\n",
      "        -6.6787e-01, -6.7940e-01, -6.7697e-01, -6.7492e-01, -6.7327e-01],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Episode 550/1000, Loss: -0.276, Avg Reward: 8.83\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4435,  1.0910,  0.7350,  0.3755,  0.0122, -0.3547, -0.7253, -1.0996,\n",
      "        -1.4777])\n",
      "tensor([-1.2779e-04, -5.4193e-04, -2.4529e-03, -1.7425e-02, -1.8365e-01,\n",
      "        -6.8313e-01, -6.8895e-01, -6.8721e-01, -6.8508e-01],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Episode 600/1000, Loss: -0.278, Avg Reward: 8.83\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4145,  1.0185,  0.6184,  0.2143, -0.1938, -0.6061, -1.0226, -1.4432])\n",
      "tensor([-1.2207e-04, -5.3525e-04, -2.6965e-03, -2.2269e-02, -2.4446e-01,\n",
      "        -6.7500e-01, -6.7358e-01, -6.7102e-01], grad_fn=<StackBackward0>)\n",
      "Episode 650/1000, Loss: -0.263, Avg Reward: 9.01\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4435,  1.0910,  0.7350,  0.3755,  0.0122, -0.3547, -0.7253, -1.0996,\n",
      "        -1.4777])\n",
      "tensor([-7.6771e-05, -3.4451e-04, -1.7009e-03, -1.2948e-02, -1.5219e-01,\n",
      "        -6.8662e-01, -6.7578e-01, -6.7655e-01, -6.7753e-01],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Episode 700/1000, Loss: -0.274, Avg Reward: 9.19\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4664,  1.1492,  0.8289,  0.5052,  0.1784, -0.1518, -0.4853, -0.8222,\n",
      "        -1.1625, -1.5062])\n",
      "tensor([-4.9114e-05, -2.2125e-04, -1.0705e-03, -7.6270e-03, -8.6856e-02,\n",
      "        -6.4402e-01, -6.8564e-01, -6.8607e-01, -6.8637e-01, -6.8654e-01],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Episode 750/1000, Loss: -0.281, Avg Reward: 8.83\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4664,  1.1492,  0.8289,  0.5052,  0.1784, -0.1518, -0.4853, -0.8222,\n",
      "        -1.1625, -1.5062])\n",
      "tensor([-3.5286e-05, -1.6403e-04, -8.3089e-04, -6.1340e-03, -7.2952e-02,\n",
      "        -6.4639e-01, -6.8174e-01, -6.8073e-01, -6.7992e-01, -6.7932e-01],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Episode 800/1000, Loss: -0.279, Avg Reward: 9.38\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4145,  1.0185,  0.6184,  0.2143, -0.1938, -0.6061, -1.0226, -1.4432])\n",
      "tensor([-4.4346e-05, -2.3603e-04, -1.4412e-03, -1.4695e-02, -2.2897e-01,\n",
      "        -6.8650e-01, -6.8484e-01, -6.8302e-01], grad_fn=<StackBackward0>)\n",
      "Episode 850/1000, Loss: -0.268, Avg Reward: 9.01\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4664,  1.1492,  0.8289,  0.5052,  0.1784, -0.1518, -0.4853, -0.8222,\n",
      "        -1.1625, -1.5062])\n",
      "tensor([-2.2888e-05, -1.1492e-04, -6.3872e-04, -5.2116e-03, -6.8836e-02,\n",
      "        -6.4801e-01, -6.8848e-01, -6.8808e-01, -6.8781e-01, -6.8763e-01],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Episode 900/1000, Loss: -0.282, Avg Reward: 8.65\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4664,  1.1492,  0.8289,  0.5052,  0.1784, -0.1518, -0.4853, -0.8222,\n",
      "        -1.1625, -1.5062])\n",
      "tensor([-2.3365e-05, -1.2445e-04, -7.6652e-04, -7.3111e-03, -1.1541e-01,\n",
      "        -6.8724e-01, -6.9294e-01, -6.9249e-01, -6.9224e-01, -6.9219e-01],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Episode 950/1000, Loss: -0.283, Avg Reward: 8.83\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([ 1.4664,  1.1492,  0.8289,  0.5052,  0.1784, -0.1518, -0.4853, -0.8222,\n",
      "        -1.1625, -1.5062])\n",
      "tensor([-1.1921e-05, -6.8188e-05, -4.1461e-04, -3.7050e-03, -5.7196e-02,\n",
      "        -6.8159e-01, -6.7764e-01, -6.7671e-01, -6.7599e-01, -6.7552e-01],\n",
      "       grad_fn=<StackBackward0>)\n",
      "Episode 1000/1000, Loss: -0.278, Avg Reward: 8.83\n"
     ]
    }
   ],
   "source": [
    "def train_REINFORCE(\n",
    "    env_name=\"CartPole-v1\",\n",
    "    device=\"cpu\",\n",
    "    num_episodes=1000,\n",
    "    gamma=0.99,\n",
    "    lr=5e-4,\n",
    "    log_interval=50,\n",
    "):\n",
    "\n",
    "    # Environment\n",
    "    train_env = make_env(env_name, device=device)\n",
    "    test_env = make_env(env_name, device=device)\n",
    "\n",
    "    # Actor\n",
    "    actor = make_REINFORCE_module(env_name, device).to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(actor.parameters(), lr=lr)\n",
    "\n",
    "    # Training\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        actor.train()\n",
    "\n",
    "        # Rollout one episode\n",
    "        td = train_env.rollout(\n",
    "            policy=actor,\n",
    "            auto_reset=True,\n",
    "            auto_cast_to_device=True,\n",
    "            break_when_any_done=True,\n",
    "            max_steps=500\n",
    "        )\n",
    "\n",
    "        rewards = td[\"next\", \"reward\"].squeeze(-1)  # [T]\n",
    "        \n",
    "        log_probs = td.get(\"action_log_prob\")  # from SafeProbabilisticModule\n",
    "        T = rewards.shape[0]\n",
    "\n",
    "        # Compute discounted returns\n",
    "        returns = torch.zeros(T, device=device)\n",
    "        G = 0\n",
    "        for t in reversed(range(T)):\n",
    "            G = rewards[t] + gamma * G\n",
    "            returns[t] = G\n",
    "\n",
    "        # Normalize returns (optional, stabilizes training)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        # Compute REINFORCE loss\n",
    "        loss = -(log_probs.squeeze(-1) * returns).mean()\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging\n",
    "        if episode % log_interval == 0:\n",
    "            print(rewards)\n",
    "            print(returns)\n",
    "            print(log_probs)\n",
    "            avg_reward = eval_model(actor, test_env, num_episodes=5)\n",
    "            print(f\"Episode {episode}/{num_episodes}, Loss: {loss.item():.3f}, Avg Reward: {avg_reward:.2f}\")\n",
    "\n",
    "    return actor\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_actor = train_REINFORCE(env_name=\"CartPole-v1\", device=\"cpu\", num_episodes=1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "0de94097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of TensorDictSequential(\n",
       "    module=ModuleList(\n",
       "      (0): TensorDictModule(\n",
       "          module=MLP(\n",
       "            (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=128, out_features=2, bias=True)\n",
       "          ),\n",
       "          device=cpu,\n",
       "          in_keys=['observation'],\n",
       "          out_keys=['logits'])\n",
       "      (1): SafeProbabilisticModule(\n",
       "          in_keys=['logits'],\n",
       "          out_keys=['action', 'action_log_prob'],\n",
       "          distribution_class=<class 'torch.distributions.categorical.Categorical'>, \n",
       "          distribution_kwargs={}),\n",
       "          default_interaction_type=deterministic),\n",
       "          num_samples=None))\n",
       "    ),\n",
       "    device=cpu,\n",
       "    in_keys=['observation'],\n",
       "    out_keys=['logits', 'action', 'action_log_prob'])>"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e854de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-hedging3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
